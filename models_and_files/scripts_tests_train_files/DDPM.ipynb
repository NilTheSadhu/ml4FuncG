{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3992c037-8124-4430-b519-599d1d232372",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd \"/workspace\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1867d77-768c-4803-a0eb-976d713c90fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192addeb-eb79-45a5-9dd2-19f17ee4ff82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -r \"requirements.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4f245ff-3242-4d5a-81c5-98c9b8fb7dc9",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 135\u001b[0m\n\u001b[1;32m    133\u001b[0m image_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m313\u001b[39m\n\u001b[1;32m    134\u001b[0m channels \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m502\u001b[39m\n\u001b[0;32m--> 135\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mDDPM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 98\u001b[0m, in \u001b[0;36mDDPM.__init__\u001b[0;34m(self, image_size, channels, timesteps)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_size \u001b[38;5;241m=\u001b[39m image_size\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# Use UNet directly\u001b[39;00m\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munet \u001b[38;5;241m=\u001b[39m \u001b[43mUNet\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchannels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchannels\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 68\u001b[0m, in \u001b[0;36mUNet.__init__\u001b[0;34m(self, in_channels, out_channels, base_channels)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder1 \u001b[38;5;241m=\u001b[39m ResidualBlock(in_channels, base_channels)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder2 \u001b[38;5;241m=\u001b[39m ResidualBlock(base_channels, base_channels \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 68\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder3 \u001b[38;5;241m=\u001b[39m \u001b[43mResidualBlock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_channels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_channels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmiddle \u001b[38;5;241m=\u001b[39m ResidualBlock(base_channels \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m4\u001b[39m, base_channels \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m4\u001b[39m, use_attention\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder3 \u001b[38;5;241m=\u001b[39m ResidualBlock(base_channels \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m4\u001b[39m, base_channels \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 30\u001b[0m, in \u001b[0;36mResidualBlock.__init__\u001b[0;34m(self, in_channels, out_channels, use_attention)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mConv2d(in_channels, out_channels, kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mGroupNorm(\u001b[38;5;241m8\u001b[39m, out_channels)\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2 \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mGroupNorm(\u001b[38;5;241m8\u001b[39m, out_channels)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Residual projection for matching dimensions\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:447\u001b[0m, in \u001b[0;36mConv2d.__init__\u001b[0;34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias, padding_mode, device, dtype)\u001b[0m\n\u001b[1;32m    445\u001b[0m padding_ \u001b[38;5;241m=\u001b[39m padding \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(padding, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m _pair(padding)\n\u001b[1;32m    446\u001b[0m dilation_ \u001b[38;5;241m=\u001b[39m _pair(dilation)\n\u001b[0;32m--> 447\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m    \u001b[49m\u001b[43min_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pair\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfactory_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:141\u001b[0m, in \u001b[0;36m_ConvNd.__init__\u001b[0;34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, bias, padding_mode, device, dtype)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_parameter(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 141\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:147\u001b[0m, in \u001b[0;36m_ConvNd.reset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset_parameters\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;66;03m# Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\u001b[39;00m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;66;03m# uniform(-1/sqrt(k), 1/sqrt(k)), where k = weight.size(1) * prod(*kernel_size)\u001b[39;00m\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;66;03m# For more details see: https://github.com/pytorch/pytorch/issues/15314#issuecomment-477448573\u001b[39;00m\n\u001b[0;32m--> 147\u001b[0m     \u001b[43minit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkaiming_uniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    149\u001b[0m         fan_in, _ \u001b[38;5;241m=\u001b[39m init\u001b[38;5;241m.\u001b[39m_calculate_fan_in_and_fan_out(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/init.py:459\u001b[0m, in \u001b[0;36mkaiming_uniform_\u001b[0;34m(tensor, a, mode, nonlinearity, generator)\u001b[0m\n\u001b[1;32m    457\u001b[0m bound \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m3.0\u001b[39m) \u001b[38;5;241m*\u001b[39m std  \u001b[38;5;66;03m# Calculate uniform bounds from standard deviation\u001b[39;00m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 459\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbound\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# === Positional Encoding === #\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, height, width, channels):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.channels = channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Create positional encodings\n",
    "        batch_size, _, h, w = x.size()\n",
    "        pe_x = torch.linspace(0, 1, self.width, device=x.device).unsqueeze(0).repeat(h, 1)\n",
    "        pe_y = torch.linspace(0, 1, self.height, device=x.device).unsqueeze(1).repeat(1, w)\n",
    "        pe_x = pe_x.unsqueeze(0).unsqueeze(0).expand(batch_size, 1, h, w)\n",
    "        pe_y = pe_y.unsqueeze(0).unsqueeze(0).expand(batch_size, 1, h, w)\n",
    "        return torch.cat([x, pe_x, pe_y], dim=1)\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, use_attention=False):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.use_attention = use_attention\n",
    "\n",
    "        # Convolutional path\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=5, padding=2)\n",
    "        self.norm1 = nn.GroupNorm(8, out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=5, padding=2)\n",
    "        self.norm2 = nn.GroupNorm(8, out_channels)\n",
    "\n",
    "        # Residual projection for matching dimensions\n",
    "        self.residual_projection = (\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "            if in_channels != out_channels\n",
    "            else nn.Identity()\n",
    "        )\n",
    "\n",
    "        if use_attention:\n",
    "            self.attention = nn.MultiheadAttention(out_channels, num_heads=4, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Project residual if necessary\n",
    "        residual = self.residual_projection(x)\n",
    "\n",
    "        # Main convolution path\n",
    "        x = F.relu(self.norm1(self.conv1(x)))\n",
    "        x = self.norm2(self.conv2(x))\n",
    "\n",
    "        # Optional attention\n",
    "        if self.use_attention:\n",
    "            batch, channels, height, width = x.shape\n",
    "            x_flat = x.view(batch, channels, height * width).permute(0, 2, 1)  # Flatten for attention\n",
    "            x = self.attention(x_flat, x_flat, x_flat)[0]\n",
    "            x = x.permute(0, 2, 1).view(batch, channels, height, width)\n",
    "\n",
    "        return F.relu(x + residual)\n",
    "\n",
    "\n",
    "# === Adjusted U-Net === #\n",
    "# === Adjusted U-Net Decoder === #\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, base_channels=512):\n",
    "        super(UNet, self).__init__()\n",
    "        self.encoder1 = ResidualBlock(in_channels, base_channels)\n",
    "        self.encoder2 = ResidualBlock(base_channels, base_channels * 2)\n",
    "        self.encoder3 = ResidualBlock(base_channels * 2, base_channels * 4)\n",
    "\n",
    "        self.middle = ResidualBlock(base_channels * 4, base_channels * 4, use_attention=True)\n",
    "\n",
    "        self.decoder3 = ResidualBlock(base_channels * 4, base_channels * 2)\n",
    "        self.decoder2 = ResidualBlock(base_channels * 2, base_channels)\n",
    "        self.decoder1 = nn.Conv2d(base_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc2 = self.encoder2(F.avg_pool2d(enc1, kernel_size=2))\n",
    "        enc3 = self.encoder3(F.avg_pool2d(enc2, kernel_size=2))\n",
    "\n",
    "        mid = self.middle(enc3)\n",
    "\n",
    "        dec3 = self.decoder3(F.interpolate(mid, scale_factor=2, mode='bilinear', align_corners=False))\n",
    "        dec2 = self.decoder2(F.interpolate(dec3, scale_factor=2, mode='bilinear', align_corners=False))\n",
    "        dec1 = self.decoder1(F.interpolate(dec2, size=x.size()[-2:], mode='bilinear', align_corners=False))  # Match input size\n",
    "\n",
    "        return dec1\n",
    "\n",
    "\n",
    "# === Diffusion Model with Adjustments === #\n",
    "class DDPM(nn.Module):\n",
    "    def __init__(self, image_size=313, channels=502, timesteps=1000):\n",
    "        super(DDPM, self).__init__()\n",
    "        self.timesteps = timesteps\n",
    "        self.image_size = image_size\n",
    "\n",
    "        # Use UNet directly\n",
    "        self.unet = UNet(in_channels=channels, out_channels=channels)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        # Forward pass through U-Net\n",
    "        return self.unet(x)\n",
    "\n",
    "class DiffusionTraining:\n",
    "    def __init__(self, model, timesteps=1000, beta_start=0.0001, beta_end=0.02):\n",
    "        self.model = model\n",
    "        self.timesteps = timesteps\n",
    "\n",
    "        # Beta schedule\n",
    "        self.betas = torch.linspace(beta_start, beta_end, timesteps)\n",
    "        self.alphas = 1.0 - self.betas\n",
    "        self.alpha_bars = torch.cumprod(self.alphas, dim=0)\n",
    "\n",
    "    def forward_diffusion(self, x_0, t):\n",
    "        noise = torch.randn_like(x_0)\n",
    "        # Ensure alpha_bars is on the same device as t\n",
    "        alpha_bar_t = self.alpha_bars.to(t.device)[t].view(-1, 1, 1, 1)\n",
    "        return torch.sqrt(alpha_bar_t) * x_0 + torch.sqrt(1 - alpha_bar_t) * noise, noise\n",
    "\n",
    "    def train_step(self, x_0, optimizer):\n",
    "        t = torch.randint(0, self.timesteps, (x_0.size(0),), device=x_0.device)\n",
    "        x_t, noise = self.forward_diffusion(x_0, t)\n",
    "        noise_pred = self.model(x_t, t)\n",
    "        loss = F.mse_loss(noise_pred, noise)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "# === Initialize Model === #\n",
    "image_size = 313\n",
    "channels = 502\n",
    "model = DDPM(image_size, channels)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6672c851-d1f5-47e8-a5fe-046e81cc209c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "high_res_file = \"./padded_high_res_grids_3.h5\"\n",
    "with h5py.File(high_res_file, \"r\") as high_res_f:\n",
    "        high_res_data = high_res_f[\"0\"][()]\n",
    "print(high_res_data[501,100:200,100:200])\n",
    "#Testing for file corruption and encoding error! All good now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfffbec-4d5f-468d-a79c-9b9d5d57332a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6287f9f-9399-41fc-b35a-9c1f670e7a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "# === PairedGridDatasetH5 Class === #\n",
    "class PairedGridDatasetH5(Dataset):\n",
    "    def __init__(self, high_res_file, low_res_file, keys):\n",
    "        \"\"\"\n",
    "        Dataset for paired high-res and low-res grids preloaded into RAM.\n",
    "        :param high_res_file: Path to the high-res HDF5 file.\n",
    "        :param low_res_file: Path to the low-res HDF5 file.\n",
    "        :param keys: List of keys identifying the datasets to load.\n",
    "        \"\"\"\n",
    "        self.data = []\n",
    "        self.keys = keys\n",
    "\n",
    "        print(\"Preloading data into RAM...\")\n",
    "        with h5py.File(high_res_file, \"r\") as high_res_f, h5py.File(low_res_file, \"r\") as low_res_f:\n",
    "            lf=low_res_f[\"low_res\"]\n",
    "            for key in keys:\n",
    "                # Load data into memory\n",
    "                low_res_grid = lf[key][...]  # Entire low-res grid\n",
    "                high_res_grid = high_res_f[key][...]  # Entire high-res grid\n",
    "                self.data.append((low_res_grid, high_res_grid))\n",
    "        print(\"Data preloaded.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Access preloaded data and create a mask dynamically.\n",
    "        \"\"\"\n",
    "        low_res_grid, high_res_grid = self.data[idx]\n",
    "\n",
    "        # Convert grids to PyTorch tensors\n",
    "        low_res_grid = torch.tensor(low_res_grid, dtype=torch.float32)\n",
    "        high_res_grid = torch.tensor(high_res_grid, dtype=torch.float32)\n",
    "\n",
    "        # Create mask\n",
    "        mask = (high_res_grid[0] != -1).float()  # Use the first channel as reference\n",
    "        mask = mask.unsqueeze(0).expand(502, -1, -1)  # Match 502 channels\n",
    "        return low_res_grid, high_res_grid, mask\n",
    "        self.keys = keys\n",
    "\n",
    "# === File Paths === #\n",
    "high_res_file = \"./padded_high_res_grids_3.h5\"\n",
    "low_res_file = \"./padded_intermediate_BAYESSPACE.h5\"\n",
    "\n",
    "# === Load Keys and Shuffle === #\n",
    "with h5py.File(high_res_file, \"r\") as h5f:\n",
    "    all_keys = list(h5f.keys())  #The keys match between high_res and low_res files\n",
    "\n",
    "# Shuffle keys\n",
    "np.random.shuffle(all_keys)\n",
    "\n",
    "# === Train/Val/Test Split === #\n",
    "train_ratio, val_ratio = 0.8, 0.1\n",
    "train_keys = all_keys[:int(train_ratio * len(all_keys))]\n",
    "val_keys = all_keys[int(train_ratio * len(all_keys)):int((train_ratio + val_ratio) * len(all_keys))]\n",
    "test_keys = all_keys[int((train_ratio + val_ratio) * len(all_keys)):]\n",
    "\n",
    "# === Create Datasets === #\n",
    "train_dataset = PairedGridDatasetH5(high_res_file, low_res_file, train_keys)\n",
    "val_dataset = PairedGridDatasetH5(high_res_file, low_res_file, val_keys)\n",
    "test_dataset = PairedGridDatasetH5(high_res_file, low_res_file, test_keys)\n",
    "# === Create DataLoaders === #\n",
    "batch_size = 5\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "# === Sanity Check: Verify Data Loading === #\n",
    "print(f\"Train set size: {len(train_dataset)}\")\n",
    "print(f\"Validation set size: {len(val_dataset)}\")\n",
    "print(f\"Test set size: {len(test_dataset)}\")\n",
    "for batch in train_loader:\n",
    "    low_res, high_res, mask = batch\n",
    "    print(f\"Low-res shape: {low_res.shape}\")\n",
    "    print(f\"High-res shape: {high_res.shape}\")\n",
    "    if mask is not None:\n",
    "        print(f\"Mask shape: {mask.shape}\")\n",
    "    break\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dbf87b-c136-4f50-a975-807325f09be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEBUGGING CELL\n",
    "with h5py.File(high_res_file, \"r\") as high_res_f, h5py.File(low_res_file, \"r\") as low_res_f:\n",
    "            lf=low_res_f[\"low_res\"]\n",
    "            print(lf[\"0\"][...])\n",
    "            #no difference between () and ... for conversion time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2a335f-7338-4424-acc2-444d152139eb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize DDPM model\n",
    "from torch.optim import Adam\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "num_epochs=20\n",
    "image_size = 313\n",
    "input_channels = 502\n",
    "model = DDPM(image_size, input_channels).to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = Adam(model.parameters(), lr=5e-4)\n",
    "\n",
    "# Diffusion Framework for Training\n",
    "trainer = DiffusionTraining(model)\n",
    "\n",
    "# Checkpoint path\n",
    "checkpoint_path = '/content/drive/MyDrive/checkpoints/ddpm_checkpoint.pth'\n",
    "\n",
    "\n",
    "def save_checkpoint(epoch, model, optimizer, train_losses, val_losses, checkpoint_path):\n",
    "    \"\"\"\n",
    "    Save training checkpoint.\n",
    "    :param epoch: Current epoch.\n",
    "    :param model: DDPM model instance.\n",
    "    :param optimizer: Optimizer instance.\n",
    "    :param train_losses: List of training losses.\n",
    "    :param val_losses: List of validation losses.\n",
    "    :param checkpoint_path: Path to save the checkpoint.\n",
    "    \"\"\"\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "    }, checkpoint_path)\n",
    "\n",
    "\n",
    "# Training loop with memory optimizations\n",
    "accumulation_steps = 4  # Simulate larger batch size by accumulating gradients\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    epoch_train_loss = 0\n",
    "\n",
    "    for i, (lr_data, hr_data, mask) in enumerate(train_loader):\n",
    "        # Move data to GPU when needed\n",
    "        lr_data = lr_data.to(device, non_blocking=True)\n",
    "        hr_data = hr_data.to(device, non_blocking=True)\n",
    "        mask = mask.to(device, non_blocking=True)\n",
    "\n",
    "        # Forward diffusion process\n",
    "        t = torch.randint(0, trainer.timesteps, (lr_data.size(0),), device=device)\n",
    "        x_t, noise = trainer.forward_diffusion(hr_data, t)\n",
    "\n",
    "        # Predict noise\n",
    "        noise_pred = model(x_t, t)\n",
    "\n",
    "        # Masked loss calculation\n",
    "        loss = F.mse_loss(noise_pred * mask, noise * mask, reduction='sum') / mask.sum()\n",
    "\n",
    "        # Normalize loss for accumulation\n",
    "        loss = loss / accumulation_steps\n",
    "        loss.backward()\n",
    "\n",
    "        # Perform optimizer step after accumulating gradients\n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        epoch_train_loss += loss.item()\n",
    "\n",
    "        # Free GPU memory\n",
    "        del lr_data, hr_data, mask, x_t, noise, noise_pred\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Final step for remaining gradients (if total batches are not divisible by accumulation_steps)\n",
    "    if len(train_loader) % accumulation_steps != 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # Log training loss\n",
    "    avg_train_loss = epoch_train_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    print(f\"Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    epoch_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for lr_data, hr_data, mask in val_loader:\n",
    "            lr_data = lr_data.to(device, non_blocking=True)\n",
    "            hr_data = hr_data.to(device, non_blocking=True)\n",
    "            mask = mask.to(device, non_blocking=True)\n",
    "\n",
    "            # Forward diffusion process\n",
    "            t = torch.randint(0, trainer.timesteps, (lr_data.size(0),), device=device)\n",
    "            x_t, noise = trainer.forward_diffusion(hr_data, t)\n",
    "            noise_pred = model(x_t, t)\n",
    "\n",
    "            # Masked validation loss\n",
    "            val_loss = F.mse_loss(noise_pred * mask, noise * mask, reduction='sum') / mask.sum()\n",
    "            epoch_val_loss += val_loss.item()\n",
    "\n",
    "            # Free GPU memory\n",
    "            del lr_data, hr_data, mask, x_t, noise, noise_pred\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    avg_val_loss = epoch_val_loss / len(val_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Save checkpoint\n",
    "    save_checkpoint(epoch, model, optimizer, train_losses, val_losses, checkpoint_path)\n",
    "\n",
    "# Test evaluation\n",
    "model.eval()\n",
    "epoch_test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for lr_data, hr_data, mask in test_loader:\n",
    "        lr_data = lr_data.to(device, non_blocking=True)\n",
    "        hr_data = hr_data.to(device, non_blocking=True)\n",
    "        mask = mask.to(device, non_blocking=True)\n",
    "\n",
    "        # Forward diffusion process\n",
    "        t = torch.randint(0, trainer.timesteps, (lr_data.size(0),), device=device)\n",
    "        x_t, noise = trainer.forward_diffusion(hr_data, t)\n",
    "        noise_pred = model(x_t, t)\n",
    "\n",
    "        # Masked test loss\n",
    "        test_loss = F.mse_loss(noise_pred * mask, noise * mask, reduction='sum') / mask.sum()\n",
    "        epoch_test_loss += test_loss.item()\n",
    "\n",
    "        # Free GPU memory\n",
    "        del lr_data, hr_data, mask, x_t, noise, noise_pred\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "avg_test_loss = epoch_test_loss / len(test_loader)\n",
    "print(f\"Test Loss: {avg_test_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5b3543-f538-424c-939b-2b8c31823bcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c7ca946b-9601-4066-9d0d-9631b85dd354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State_dict successfully remapped and loaded.\n",
      "Models are functional\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "# === Utility Functions === #\n",
    "def align_tensor_sizes(tensor_a, tensor_b):\n",
    "    \"\"\"Ensure tensor_a matches the dimensions of tensor_b.\"\"\"\n",
    "    _, _, height_t1, width_t1 = tensor_a.size()\n",
    "    _, _, height_t2, width_t2 = tensor_b.size()\n",
    "\n",
    "    if height_t1 != height_t2 or width_t1 != width_t2:\n",
    "        tensor_a = F.interpolate(tensor_a, size=(height_t2, width_t2), mode='bilinear', align_corners=False)\n",
    "\n",
    "    return tensor_a\n",
    "\n",
    "# === ResNetUNet Definition === #\n",
    "class ResNetUNet(nn.Module):\n",
    "    def __init__(self, pretrained_encoder=None, out_channels=502):\n",
    "        super(ResNetUNet, self).__init__()\n",
    "\n",
    "        # Use the provided InterpolationModel as the encoder\n",
    "        self.encoder = pretrained_encoder\n",
    "\n",
    "        # Decoder layers\n",
    "        self.decoder4 = nn.ConvTranspose2d(512, 256, kernel_size=5, stride=2, padding=1)\n",
    "        self.decoder3 = nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1)\n",
    "        self.decoder2 = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.decoder1 = nn.ConvTranspose2d(64, out_channels, kernel_size=2, stride=2, padding=1)\n",
    "\n",
    "        # Skip connections\n",
    "        self.skip4 = nn.Conv2d(256, 256, kernel_size=1)\n",
    "        self.skip3 = nn.Conv2d(128, 128, kernel_size=1)\n",
    "        self.skip2 = nn.Conv2d(64, 64, kernel_size=1)\n",
    "\n",
    "    def forward(self, x, original_size):\n",
    "        # Pass input through the InterpolationModel encoder\n",
    "        enc1 = self.encoder.encoder.conv1(x)  # Initial convolution\n",
    "        enc1 = self.encoder.encoder.bn1(enc1)  # Batch normalization\n",
    "        enc1 = F.relu(enc1)  # Add ReLU activation explicitly\n",
    "\n",
    "        enc2 = self.encoder.encoder.layer1(enc1)  # Residual layer 1\n",
    "        enc3 = self.encoder.encoder.layer2(enc2)  # Residual layer 2\n",
    "        enc4 = self.encoder.encoder.layer3(enc3)  # Residual layer 3\n",
    "\n",
    "        # Bottleneck\n",
    "        bottleneck = self.encoder.encoder.layer4(enc4)  # Residual layer 4\n",
    "\n",
    "        # Decoder with skip connections\n",
    "        dec4 = self.decoder4(bottleneck)\n",
    "        dec4 = align_tensor_sizes(dec4, self.skip4(enc4)) + self.skip4(enc4)\n",
    "\n",
    "        dec3 = self.decoder3(dec4)\n",
    "        dec3 = align_tensor_sizes(dec3, self.skip3(enc3)) + self.skip3(enc3)\n",
    "\n",
    "        dec2 = self.decoder2(dec3)\n",
    "        dec2 = align_tensor_sizes(dec2, self.skip2(enc2)) + self.skip2(enc2)\n",
    "\n",
    "        dec1 = self.decoder1(dec2)\n",
    "\n",
    "        # Final output resizing to match original size\n",
    "        return dec1[:, :, :original_size[0], :original_size[1]]\n",
    "\n",
    "# === Function to Load and Remap State_dict === #\n",
    "def load_and_remap_state_dict(model, checkpoint_path):\n",
    "    state_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "    new_state_dict = {}\n",
    "    for key, value in state_dict.items():\n",
    "        if key.startswith(\"decoder.\"):\n",
    "            # Map decoder keys\n",
    "            new_key = key.replace(\"decoder.0\", \"decoder4\").replace(\"decoder.2\", \"decoder3\") \\\n",
    "                         .replace(\"decoder.4\", \"decoder2\").replace(\"decoder.6\", \"decoder1\")\n",
    "        elif key.startswith(\"skip\"):\n",
    "            # Map skip connection keys\n",
    "            new_key = key.replace(\"skip\", \"skip\")\n",
    "        elif key == \"encoder.conv1.weight\":\n",
    "            # Handle conv1 shape mismatch\n",
    "            new_key = key\n",
    "        else:\n",
    "            # Keep other keys unchanged\n",
    "            new_key = key\n",
    "        new_state_dict[new_key] = value\n",
    "\n",
    "    # Load remapped state_dict\n",
    "    model.load_state_dict(new_state_dict, strict=False)\n",
    "    print(\"State_dict successfully remapped and loaded.\")\n",
    "\n",
    "# === Initialize Model and Load Weights === #\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "resnet=InterpolationModel()\n",
    "resnet.to(device)\n",
    "# Path to checkpoint\n",
    "checkpoint_path = './interpolation_model.pth'\n",
    "load_and_remap_state_dict(resnet, checkpoint_path)\n",
    "model = ResNetUNet(out_channels=502,pretrained_encoder=resnet)\n",
    "model.to(device)\n",
    "\n",
    "print(\"Models are functional\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03d9c40-1b06-4dcf-abb2-097235baf1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from tasks.spatial_transcriptomics_dataset import MemoryEfficientPairedLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c3605984-2543-49d3-aec5-9934a89c131b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating loaders\n",
      "Created loaders\n",
      "Epoch 1/10\n",
      "Train Loss: 46735657.4000\n",
      "Validation Loss: 0.3457\n",
      "Epoch 2/10\n",
      "Train Loss: 40633651.8000\n",
      "Validation Loss: 0.3237\n",
      "Epoch 3/10\n",
      "Train Loss: 37912029.2000\n",
      "Validation Loss: 0.3064\n",
      "Epoch 4/10\n",
      "Train Loss: 37266074.8000\n",
      "Validation Loss: 0.3027\n",
      "Epoch 5/10\n",
      "Train Loss: 36937856.7000\n",
      "Validation Loss: 0.3048\n",
      "Epoch 6/10\n",
      "Train Loss: 35986738.8000\n",
      "Validation Loss: 0.2943\n",
      "Epoch 7/10\n",
      "Train Loss: 36441739.4000\n",
      "Validation Loss: 0.2967\n",
      "Epoch 8/10\n",
      "Train Loss: 36183274.8000\n",
      "Validation Loss: 0.2996\n",
      "Epoch 9/10\n",
      "Train Loss: 35351123.1000\n",
      "Validation Loss: 0.3052\n",
      "Epoch 10/10\n",
      "Train Loss: 35606189.0000\n",
      "Validation Loss: 0.2860\n",
      "Test Loss: 0.3157\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_high_res_files = [f\"binned_data/high_res_grids_{i}.npz\" for i in [4, 12,  14, 22]]\n",
    "train_low_res_files = [f\"binned_data/low_res_grids_{i}.npz\" for i in [4, 12,  14, 22]]\n",
    "val_high_res_files = [f\"binned_data/high_res_grids_{i}.npz\" for i in [9, 27]]\n",
    "val_low_res_files = [f\"binned_data/low_res_grids_{i}.npz\" for i in [9, 27]]\n",
    "test_high_res_files = [f\"binned_data/high_res_grids_{i}.npz\" for i in [24, 2]]\n",
    "test_low_res_files = [f\"binned_data/low_res_grids_{i}.npz\" for i in [24, 2]]\n",
    "\n",
    "#utilizing my lazy loaders\n",
    "print(\"Creating loaders\")\n",
    "train_loader = MemoryEfficientPairedLoader(train_high_res_files, train_low_res_files, batch_size=5)\n",
    "val_loader = MemoryEfficientPairedLoader(val_high_res_files, val_low_res_files, batch_size=5)\n",
    "test_loader = MemoryEfficientPairedLoader(test_high_res_files, test_low_res_files, batch_size=5)\n",
    "print(\"Created loaders\")\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=3e-4) #pick higher start\n",
    "\n",
    "#saving function copy pasted from above\n",
    "def save_checkpoint(epoch, model, optimizer, train_losses, val_losses, checkpoint_path):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "    }, checkpoint_path)\n",
    "\n",
    "#train and validate\n",
    "num_epochs = 10\n",
    "train_losses, val_losses = [], []\n",
    "checkpoint_path = 'checkpoints/resnet_unet_checkpoint.pth'\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    model.train()\n",
    "    epoch_train_loss = 0\n",
    "\n",
    "    for sample in train_loader:\n",
    "        lr_data = sample['low_res_tensor'].to(device)\n",
    "        hr_data = sample['high_res_tensor'].to(device)\n",
    "        mask = sample['mask'].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        noise_pred = model(lr_data, original_size=(313, 313))\n",
    "        noise_pred = align_tensor_sizes(noise_pred, mask) #dim align\n",
    "\n",
    "        #masked loss\n",
    "        total_loss = 0.0\n",
    "        for i in range(noise_pred.shape[0]):\n",
    "            mask2 = mask[i:i + 1]\n",
    "            loss = F.mse_loss(noise_pred[i] * mask2, hr_data[i] * mask2, reduction='sum')\n",
    "            total_loss += loss\n",
    "\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        epoch_train_loss += total_loss.item()\n",
    "\n",
    "    avg_train_loss = epoch_train_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    print(f\"Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    epoch_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            lr_data = batch['low_res_tensor'].to(device)\n",
    "            hr_data = batch['high_res_tensor'].to(device)\n",
    "            mask = batch['mask'].to(device)\n",
    "\n",
    "            noise_pred = model(lr_data, original_size=(313, 313))\n",
    "            noise_pred = align_tensor_sizes(noise_pred, mask)\n",
    "\n",
    "            val_loss = F.mse_loss(noise_pred * mask, hr_data * mask, reduction='sum') / mask.sum()\n",
    "            epoch_val_loss += val_loss.item()\n",
    "\n",
    "    avg_val_loss = epoch_val_loss / len(val_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Save checkpoint\n",
    "    save_checkpoint(epoch, model, optimizer, train_losses, val_losses, checkpoint_path)\n",
    "\n",
    "# Test evaluation\n",
    "model.eval()\n",
    "epoch_test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        lr_data = batch['low_res_tensor'].to(device)\n",
    "        hr_data = batch['high_res_tensor'].to(device)\n",
    "        mask = batch['mask'].to(device)\n",
    "\n",
    "        noise_pred = model(lr_data, original_size=(313, 313))\n",
    "        noise_pred = align_tensor_sizes(noise_pred, mask)\n",
    "\n",
    "        test_loss = F.mse_loss(noise_pred * mask, hr_data * mask, reduction='sum') / mask.sum()\n",
    "        epoch_test_loss += test_loss.item()\n",
    "\n",
    "avg_test_loss = epoch_test_loss / len(test_loader)\n",
    "print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
    "\n",
    "# Save final checkpoint\n",
    "save_checkpoint(num_epochs, model, optimizer, train_losses, val_losses, checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80ab39f8-5e3c-4f1b-b437-a21a47aec2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.55.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (165 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.1/165.1 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.7-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Downloading matplotlib-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m107.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (324 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.0/325.0 kB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.55.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m122.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.7-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m150.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.1 cycler-0.12.1 fonttools-4.55.3 kiwisolver-1.4.7 matplotlib-3.10.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28db288-a89a-43ae-bb8b-494a83eb8f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"binned_data/low_res_grids_1.npz\") #Pick a file we have not used \n",
    "low_res_grid = data[\"grids\"][4]  # Extract the specific grid\n",
    "mask2=(low_res_grid[200]>0)\n",
    "low_res_tensor = torch.tensor(low_res_grid, dtype=torch.float32).unsqueeze(0).to(device)  # [1, 502, 313, 313]\n",
    "\n",
    "#inference/superresolve the lr image\n",
    "model.eval()\n",
    "with torch.no_grad():#disable gradient to avoid unnecessary computations\n",
    "    high_res_pred = model(x=low_res_tensor, original_size=(313, 313))\n",
    "grid=np.array(high_res_pred.cpu())\n",
    "\n",
    "mask = (grid[0,200,:,:] > 0)\n",
    "channel_data = grid[0,200, :, :]*mask*mask2\n",
    "plt.figure(figsize=(10, 10))\n",
    "myim=plt.imshow(channel_data, cmap='viridis', interpolation='nearest')\n",
    "cbar = plt.colorbar(myim, label=f'Channel {501} Intensity', shrink=0.8)\n",
    "\n",
    "cbar.ax.tick_params(labelsize=12)  # Make color bar labels larger\n",
    "cbar.set_label(f'Channel {200} Intensity', fontsize=14)  # Color bar title larger\n",
    "\n",
    "plt.title(f'Visualization of ResUnet #10_256_9E4 Upscaled Synthetic Visium Gene Expression Values for: Spatial Coord X #{201}', fontsize=12)\n",
    "plt.xlabel('X Index', fontsize=14)\n",
    "plt.ylabel('Y Index', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6668cebe-fb14-483f-a330-5c68824ca428",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"binned_data/low_res_grids_28.npz\") #Pick a file we have not used \n",
    "low_res_grid = data[\"grids\"][4]  # Extract the specific grid\n",
    "mask2=(low_res_grid[200]>0)\n",
    "low_res_tensor = torch.tensor(low_res_grid, dtype=torch.float32).unsqueeze(0).to(device)  # [1, 502, 313, 313]\n",
    "\n",
    "#inference/superresolve the lr image\n",
    "model.eval()\n",
    "with torch.no_grad():#disable gradient to avoid unnecessary computations\n",
    "    high_res_pred = model(x=low_res_tensor, original_size=(313, 313))\n",
    "grid=np.array(high_res_pred.cpu())\n",
    "\n",
    "mask = (grid[0,200,:,:] > 0)\n",
    "channel_data = grid[0,200, :, :]*mask*mask2\n",
    "plt.figure(figsize=(10, 10))\n",
    "myim=plt.imshow(channel_data, cmap='viridis', interpolation='nearest')\n",
    "cbar = plt.colorbar(myim, label=f'Channel {200} Intensity', shrink=0.5)\n",
    "\n",
    "cbar.ax.tick_params(labelsize=12)  # Make color bar labels larger\n",
    "cbar.set_label(f'Channel {200} Intensity', fontsize=14)  # Color bar title larger\n",
    "\n",
    "plt.title(f'Visualization of ResUnet #12_256_9E4 Upscaled Synthetic Visium Gene Expression Values for: Spatial X Cord #{200}', fontsize=12)\n",
    "plt.xlabel('X Index', fontsize=14)\n",
    "plt.ylabel('Y Index', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b5e801-569f-4b2d-a1e6-379070a7eddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.metrics import structural_similarity as ssim, peak_signal_noise_ratio as psnr, mean_squared_error as mse\n",
    "import tensorflow as tf\n",
    "def load_grid_from_npz(npz_file, grid_key):\n",
    "    #Loads grids from an npz file.\n",
    "    with np.load(npz_file, allow_pickle=True) as data:\n",
    "        return data[grid_key]\n",
    "\n",
    "def compute_metrics(high_res_tensor, low_res_tensor):\n",
    "\n",
    "    #Computes SSIM, PSNR, and MSE between high-res and low-res grids across all channels.\n",
    "\n",
    "    ssim_values = []\n",
    "    psnr_values = []\n",
    "    mse_values = []\n",
    "\n",
    "    for channel in range(502):  # Iterate over all channels\n",
    "        high_res_channel = high_res_tensor[channel, :, :]\n",
    "        low_res_channel = low_res_tensor[channel, :, :]\n",
    "        data_range = max(high_res_channel.max(), low_res_channel.max()) - min(high_res_channel.min(), low_res_channel.min())\n",
    "        channel_ssim, _ = ssim(high_res_channel, low_res_channel, data_range=data_range, full=True)\n",
    "        ssim_values.append(channel_ssim)\n",
    "\n",
    "        channel_psnr = psnr(high_res_channel, low_res_channel, data_range=data_range)\n",
    "        psnr_values.append(channel_psnr)\n",
    "\n",
    "        channel_mse = mse(high_res_channel, low_res_channel)\n",
    "        mse_values.append(channel_mse)\n",
    "    mean_ssim = np.mean(ssim_values)\n",
    "    mean_psnr = np.mean(psnr_values)\n",
    "    mean_mse = np.mean(mse_values)\n",
    "\n",
    "    return ssim_values, psnr_values, mse_values, mean_ssim, mean_psnr, mean_mse\n",
    "\n",
    "#File paths and grid index\n",
    "high_res_file = 'binned_data/high_res_grids_28.npz'\n",
    "low_res_file = 'binned_data/high_res_grids_28.npz'\n",
    "grid_key = \"grids\"\n",
    "grid_index = 4  # Example grid index\n",
    "\n",
    "#Load the grids\n",
    "high_res_grids = load_grid_from_npz(high_res_file, grid_key)\n",
    "low_res_grids = load_grid_from_npz(low_res_file, grid_key)\n",
    "\n",
    "high_res_grid = high_res_grids[grid_index]\n",
    "low_res_grid = low_res_grids[grid_index]\n",
    "mask = (low_res_grid != -1)\n",
    "high_res_grid=high_res_grid*mask\n",
    "low_res_grid=low_res_grid*mask\n",
    "ssim_values, psnr_values, mse_values, mean_ssim, mean_psnr, mean_mse = compute_metrics(high_res_grid, low_res_grid)\n",
    "ssim_values, psnr_values, mse_values, mean_ssim, mean_psnr, mean_mse = compute_metrics(high_res_grid, channel_data)\n",
    "# Display results\n",
    "print(f\"SSIM values for -30 channel: {ssim_values[100]}\")\n",
    "print(f\"Mean SSIM across all channels: {mean_ssim}\")\n",
    "print(f\"PSNR values for 30channel: {psnr_values[100]}\")\n",
    "print(f\"Mean PSNR across all channels: {mean_psnr}\")\n",
    "print(f\"MSE values for -30 channel: {mse_values[100]}\")\n",
    "print(f\"Mean MSE across all channels: {mean_mse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d9c7ff5-61f8-40c2-ab24-1c2c94b73a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InterpolationModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(InterpolationModel, self).__init__()\n",
    "        # Encoder: Use ResNet18 for feature extraction\n",
    "        self.encoder = resnet18(weights=None)  #No pretrained weights\n",
    "        self.encoder.conv1 = nn.Conv2d(502, 64, kernel_size=5, stride=2, padding=3, bias=False)\n",
    "        self.encoder.fc = nn.Identity()  #Remove FC layer for features\n",
    "\n",
    "        # Decoder: Upsample to match [313, 313]\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=5, stride=2, padding=1),  \n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),  \n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1),  \n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2, padding=1),   \n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 502, kernel_size=2, stride=1, padding=1),  \n",
    "            nn.Upsample(size=(313, 313), mode='bilinear', align_corners=True) #final upscale to [313x313]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.encoder(x)  # Extract features\n",
    "        features = features.view(features.size(0), -1, 1, 1)  # Reshape for decoding\n",
    "        output = self.decoder(features)  #Decode to high-resolution size\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "faa2f79b-2817-4f33-a8ad-772b40675547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.3.101)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b05b49-8457-4a34-9288-8b40893ef589",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
